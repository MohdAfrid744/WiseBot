{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2691d65d"
      },
      "source": [
        "# **WiseBot: Philosophical Inquiry Agent**\n",
        "\n",
        "This notebook implements a WiseBot, an AI chatbot designed to provide philosophical and religious insights by semantically searching through various holy texts via RAG and generating responses using a large language model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUXGcdgISJL8",
        "outputId": "284fe81f-5d6b-4383-90ee-72e1e4377e25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r6JOZOqsR0VT",
        "outputId": "a01128f3-5b9a-4799-f7c6-6a3a7a4b8a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.57.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.36.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2026.1.4)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c63a54d"
      },
      "source": [
        "## Pre-embedding and FAISS Index Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5wPGgjrTSX-",
        "collapsed": true,
        "outputId": "e980a260-0a04-484b-a381-6514c8485382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive path set to: /content/drive/MyDrive/WiseBot\n",
            "Loading pre-existing embeddings, metadata, and FAISS index from Google Drive...\n",
            "Pre-embedding files loaded/confirmed.\n"
          ]
        }
      ],
      "source": [
        "DRIVE_PATH = '/content/drive/MyDrive/WiseBot'\n",
        "print(f\"Google Drive path set to: {DRIVE_PATH}\")\n",
        "\n",
        "def load_datasets():\n",
        "    \"\"\"Load datasets from CSV files.\"\"\"\n",
        "    gita_df = pd.read_csv('/content/drive/MyDrive/WiseBot/gita.csv')\n",
        "    quran_df = pd.read_csv('/content/drive/MyDrive/WiseBot/quran.csv')\n",
        "    bible_df = pd.read_csv('/content/drive/MyDrive/WiseBot/bible.csv')\n",
        "    return gita_df, quran_df, bible_df\n",
        "\n",
        "def generate_embeddings(datasets, model, batch_size=32):\n",
        "    \"\"\"Generate embeddings for all verses using batch processing.\"\"\"\n",
        "    embeddings = []\n",
        "    metadata = []\n",
        "\n",
        "    for book, verses_list in datasets.items():\n",
        "        for i in range(0, len(verses_list), batch_size):\n",
        "            batch = verses_list[i : i + batch_size]\n",
        "            verses = [entry[\"verse\"] for entry in batch]\n",
        "            batch_embeddings = model.encode(verses)\n",
        "            embeddings.extend(batch_embeddings)\n",
        "            metadata.extend([\n",
        "                {\n",
        "                    \"book\": book,\n",
        "                    \"verse\": entry[\"verse\"],\n",
        "                }\n",
        "                for entry in batch\n",
        "            ])\n",
        "\n",
        "    return np.array(embeddings), metadata\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    \"\"\"Create and return a FAISS index.\"\"\"\n",
        "    if embeddings.size == 0:\n",
        "        return None\n",
        "\n",
        "    embedding_dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(embedding_dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def generate_and_save_embeddings(datasets, model):\n",
        "    \"\"\"Generate and save embeddings and metadata.\"\"\"\n",
        "    embeddings, metadata = generate_embeddings(datasets, model)\n",
        "    with open(os.path.join(DRIVE_PATH, 'embeddings.pkl'), 'wb') as f:\n",
        "        pickle.dump(embeddings, f)\n",
        "    with open(os.path.join(DRIVE_PATH, 'metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "    print(\"Embeddings and metadata saved.\")\n",
        "    return embeddings, metadata # Return embeddings for index creation\n",
        "\n",
        "def create_and_save_index(embeddings):\n",
        "    \"\"\"Create and save FAISS index.\"\"\"\n",
        "    index = create_faiss_index(embeddings)\n",
        "    with open(os.path.join(DRIVE_PATH, 'index.pkl'), 'wb') as f:\n",
        "        pickle.dump(index, f)\n",
        "    print(\"FAISS index saved.\")\n",
        "\n",
        "def pre_embed_and_save():\n",
        "    \"\"\"Pre-calculate embeddings and save to file, or load if already present.\"\"\"\n",
        "    os.makedirs(DRIVE_PATH, exist_ok=True) # Ensure the directory exists\n",
        "\n",
        "    # Check if pre-existing files are available in DRIVE_PATH\n",
        "    embeddings_path = os.path.join(DRIVE_PATH, 'embeddings.pkl')\n",
        "    metadata_path = os.path.join(DRIVE_PATH, 'metadata.pkl')\n",
        "    index_path = os.path.join(DRIVE_PATH, 'index.pkl')\n",
        "\n",
        "    if os.path.exists(embeddings_path) and os.path.exists(metadata_path) and os.path.exists(index_path):\n",
        "        print(\"Loading pre-existing embeddings, metadata, and FAISS index from Google Drive...\")\n",
        "        print(\"Pre-embedding files loaded/confirmed.\")\n",
        "    else:\n",
        "        print(\"Pre-embedding files not found in Google Drive. Generating new embeddings and FAISS index...\")\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        gita_df, quran_df, bible_df = load_datasets()\n",
        "        datasets = {\n",
        "            \"Gita\": gita_df.to_dict('records'),\n",
        "            \"Quran\": quran_df.to_dict('records'),\n",
        "            \"Bible\": bible_df.to_dict('records')\n",
        "        }\n",
        "        embeddings, _ = generate_and_save_embeddings(datasets, model) # Get embeddings for index creation\n",
        "        create_and_save_index(embeddings)\n",
        "        print(\"Pre-embedding completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pre_embed_and_save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b501a6ef"
      },
      "source": [
        "## Global Configuration, Logging, and Data Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bd196a6",
        "outputId": "4921d5a4-bdc9-4a2b-be2a-d39962f92976"
      },
      "source": [
        "DRIVE_PATH = '/content/drive/MyDrive/WiseBot'\n",
        "\n",
        "# Ensure the DRIVE_PATH exists\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "# Define CACHE_FILE path\n",
        "CACHE_FILE = os.path.join(DRIVE_PATH, 'cache.pkl')\n",
        "\n",
        "# Configure logging\n",
        "log_file_name = os.path.join(DRIVE_PATH, 'app.log')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.WARNING)\n",
        "\n",
        "handler = logging.FileHandler(log_file_name, mode='a')\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "\n",
        "# Clear existing handlers to prevent duplicate logs if cell is run multiple times\n",
        "if logger.handlers:\n",
        "    logger.handlers.clear()\n",
        "logger.addHandler(handler)\n",
        "\n",
        "logger.info(\"Application logging setup and initiated.\")\n",
        "handler.flush()\n",
        "\n",
        "def load_cache():\n",
        "    \"\"\"Loads the cache from a pickle file.\"\"\"\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        try:\n",
        "            with open(CACHE_FILE, 'rb') as f:\n",
        "                cache = pickle.load(f)\n",
        "            logger.info(f\"Cache loaded from {CACHE_FILE}\")\n",
        "            handler.flush()\n",
        "            return cache\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading cache from {CACHE_FILE}: {e}\")\n",
        "            handler.flush()\n",
        "            return {}\n",
        "    logger.info(\"Cache file not found. Starting with empty cache.\")\n",
        "    handler.flush()\n",
        "    return {}\n",
        "\n",
        "def save_cache(cache):\n",
        "    \"\"\"Saves the cache to a pickle file.\"\"\"\n",
        "    try:\n",
        "        with open(CACHE_FILE, 'wb') as f:\n",
        "            pickle.dump(cache, f)\n",
        "        logger.info(f\"Cache saved to {CACHE_FILE}\")\n",
        "        handler.flush()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving cache to {CACHE_FILE}: {e}\")\n",
        "        handler.flush()\n",
        "\n",
        "def load_preembedded_data():\n",
        "    \"\"\"Load pre-embedded data from files.\"\"\"\n",
        "    global DRIVE_PATH\n",
        "    try:\n",
        "        with open(os.path.join(DRIVE_PATH, 'embeddings.pkl'), 'rb') as f:\n",
        "            embeddings = pickle.load(f)\n",
        "        with open(os.path.join(DRIVE_PATH, 'metadata.pkl'), 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "        with open(os.path.join(DRIVE_PATH, 'index.pkl'), 'rb') as f:\n",
        "            index = pickle.load(f)\n",
        "        logger.info(\"Pre-embedded data loaded successfully.\")\n",
        "        handler.flush()\n",
        "        metadata_dict = {i: entry for i, entry in enumerate(metadata)}\n",
        "        return embeddings, metadata_dict, index\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading pre-embedded data: {e}\")\n",
        "        handler.flush()\n",
        "        raise\n",
        "\n",
        "# Initialize global variables\n",
        "cache = load_cache()\n",
        "embeddings, metadata_dict, index = load_preembedded_data()\n",
        "model_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# --- Gemini API Configuration ---\n",
        "# Fetch API key from Colab Secrets\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY);\n",
        "    gemini_model = genai.GenerativeModel('gemini-2.5-flash-lite') # Using a free, fast Gemini model\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API. Make sure GOOGLE_API_KEY is set in Colab secrets: {e}\")\n",
        "    gemini_model = None # Set to None if API key isn't configured\n",
        "# --- End Gemini API Configuration ---\n",
        "\n",
        "# --- Out-of-Scope Keywords for API Quota Saving ---\n",
        "OUT_OF_SCOPE_KEYWORDS = [\n",
        "    \"hello\", \"hlo\", \"hllo\", \"hii\", \"hi\", \"how are you\", \"what's up\", \"hey\", # Greetings\n",
        "    \"time\", \"date\", \"weather\", \"news\", \"current events\", # General knowledge/current affairs\n",
        "    \"what is the capital\", \"how many\", # Factual/general questions\n",
        "    \"tell me about yourself\", \"your name\", \"who made you\", # Questions about the bot itself\n",
        "    \"joke\", \"story\", \"fun fact\", # Entertainment\n",
        "    \"exit\", \"quit\", \"bye\", \"goodbye\" # Explicit exit commands (should already be handled but for redundancy)\n",
        "]\n",
        "# --- End Out-of-Scope Keywords ---\n",
        "\n",
        "print(\"Setup complete: Logging configured, cache and pre-embedded data loaded, SentenceTransformer model initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete: Logging configured, cache and pre-embedded data loaded, SentenceTransformer model initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5191eec"
      },
      "source": [
        "## Interactive Question Answering Logic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5010b4cb",
        "outputId": "a7b1761f-0be9-4a93-de3a-469344040296",
        "collapsed": true
      },
      "source": [
        "def get_llm_response(question, grouped_context_verses):\n",
        "    \"\"\"Gets a response from the Gemini API for a given question and context.\"\"\"\n",
        "    global gemini_model\n",
        "\n",
        "    if gemini_model is None:\n",
        "        return \"Error: Gemini API model is not initialized. Please ensure GOOGLE_API_KEY is set and the model is initialized in the global setup.\"\n",
        "\n",
        "    formatted_context_parts = []\n",
        "    for book, verses in grouped_context_verses.items():\n",
        "        if verses:\n",
        "            formatted_context_parts.append(f\"From {book}: {', '.join(verses)}\")\n",
        "        else:\n",
        "            formatted_context_parts.append(f\"From {book}: No specific verses found.\")\n",
        "    formatted_context = \"\\n\".join(formatted_context_parts)\n",
        "\n",
        "    prompt = f\"\"\"Your primary role is to act as a WiseBot, providing philosophical or religious insights based on holy texts.\\n\\n--- INSTRUCTIONS ---\\n\\n1.  **Scope Determination**:\\n    *   **IF** the user's question is a greeting, casual conversation (e.g., 'hello', 'how are you'), about current events, weather, time, general knowledge (e.g., 'who is the president?'), or any topic *not* related to philosophy, religion, spirituality, ethics, or the teachings in the holy books:\\n        *   **THEN**, you *must* respond with the exact phrase: \"This question is out of scope for my purpose.\" and provide no further answer.\\n    *   **OTHERWISE** (if the question is within scope), proceed to the next step.\\n\\n2.  **Answering based on Context**:\\n    *   The 'Context' below provides verses grouped by holy book. It will either show specific verses (e.g., \"From Gita: verse1, verse2\") or explicitly state \"No specific verses found.\".\\n    *   You *must* generate a *separate answer block* for *each* holy book mentioned in the provided 'Context', strictly following the format below.\\n\\n    a.  **For Books with Specific Verses (e.g., \"From Gita: verse1, verse2\")**:\\n        *   Your answer for that specific book *must be based solely on the provided verses for that book*.\\n        *   From the verses provided for that book, select *only the single most relevant verse* to the user's question to include in your answer block.\\n\\n    b.  **For Books with No Specific Verses (e.g., \"From Quran: No specific verses found.\")**:\\n        *   You should generate a *general philosophical answer* to the 'Question' *as it relates to the broader teachings or philosophical stance of that holy book*, even without specific verses directly from the context. This answer should reflect the general spirit of the book on the topic.\\n\\n3.  **Strict Output Format for EACH Answer Block**:\\n    *   Each answer block *must* adhere strictly to the following 3-line format. No deviations, no extra lines, no missing lines:\\n        ```\\n        According to [Book Name]:\\n        Verse: [Either the single most relevant verse from the context OR a general philosophical statement if no verses were found]\\n        Meaning: [The philosophical meaning or interpretation, based on the provided context (if verses exist), or on the general teachings of the book (if no verses exist), explained in simple, understandable language.]\\n        Example: [A short, real-life example that clearly illustrates the meaning of the verse/philosophical statement.]\\n        ```\\n\\n--- QUESTION AND CONTEXT ---\\nQuestion: {question}\\nContext from the selected holy books: {formatted_context}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Attempting to call Gemini API with model 'gemini-2.5-flash-lite' for question: {question[:50]}...\")\n",
        "        handler.flush()\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "        generated_content = response.text\n",
        "        logger.info(f\"Gemini API call successful.\")\n",
        "        handler.flush()\n",
        "        return generated_content\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error calling Gemini API for question: {question[:50]}... - {e}\")\n",
        "        handler.flush()\n",
        "        return f\"Error: Gemini API call failed: {e}\"\n",
        "\n",
        "# Refactored function to get only the user's question and handle immediate exit\n",
        "def _get_user_question_and_handle_exit():\n",
        "    \"\"\"Get user's question and handle immediate exit commands.\"\"\"\n",
        "    question = input(\"You: \")\n",
        "    if question.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        return None # Signal exit\n",
        "    return question\n",
        "\n",
        "# New function to get book choices and handle invalid input/exit\n",
        "def _get_book_choices_and_handle_exit():\n",
        "    \"\"\"Get user's book choices, handling invalid input and exit commands.\"\"\"\n",
        "    book_map = {\n",
        "        1: \"Gita\",\n",
        "        2: \"Quran\",\n",
        "        3: \"Bible\"\n",
        "    }\n",
        "\n",
        "    # The full book selection menu is explicitly removed from here to meet user requirements.\n",
        "    # It's assumed the user knows the options or can refer to an earlier static markdown cell.\n",
        "\n",
        "    while True:\n",
        "        choices_input = input(\"Enter your choices (e.g., 1,2 for Gita and Quran): \")\n",
        "\n",
        "        if choices_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            return None # Signal exit\n",
        "\n",
        "        selected_book_names = []\n",
        "        has_error = False # Flag to track any input error\n",
        "\n",
        "        try:\n",
        "            raw_choices = choices_input.split(',')\n",
        "            if not raw_choices or all(not c.strip() for c in raw_choices):\n",
        "                has_error = True\n",
        "            else:\n",
        "                for choice_str in raw_choices:\n",
        "                    choice_str = choice_str.strip()\n",
        "                    if not choice_str:\n",
        "                        continue # Skip empty strings from extra commas\n",
        "                    choice = int(choice_str)\n",
        "                    if choice in book_map:\n",
        "                        selected_book_names.append(book_map[choice])\n",
        "                    else:\n",
        "                        has_error = True\n",
        "                        break # Break and re-prompt if any choice is invalid\n",
        "\n",
        "        except ValueError:\n",
        "            has_error = True\n",
        "\n",
        "        if has_error or not selected_book_names:\n",
        "            # Print a single, consistent error message for all invalid cases.\n",
        "            print(\"Invalid input format.\")\n",
        "            logger.warning(f\"Invalid book choice input detected: {choices_input}\")\n",
        "            handler.flush()\n",
        "            # The loop continues, prompting for input again.\n",
        "        else:\n",
        "            logger.info(f\"User selected books: {selected_book_names}\")\n",
        "            handler.flush()\n",
        "            return list(dict.fromkeys(selected_book_names)) # Remove duplicates\n",
        "\n",
        "def _perform_semantic_search(question, model_encoder, index):\n",
        "    \"\"\"Performs semantic search using the query and FAISS index.\"\"\"\n",
        "    query_embedding = model_encoder.encode(question)\n",
        "    D, I = index.search(query_embedding.reshape(1, -1), k=50)\n",
        "    logger.info(f\"Semantic search performed for question: {question[:50]}...\")\n",
        "    handler.flush()\n",
        "    return D, I\n",
        "\n",
        "def _group_context_verses(search_indices, selected_book_names, metadata_dict):\n",
        "    \"\"\"Groups relevant verses by book from search results.\"\"\"\n",
        "    grouped_context_verses = {book: [] for book in selected_book_names}\n",
        "    verse_counts = {book: 0 for book in selected_book_names}\n",
        "\n",
        "    for idx in search_indices:\n",
        "        metadata_entry = metadata_dict.get(idx, {})\n",
        "        if metadata_entry and metadata_entry.get('book') in selected_book_names:\n",
        "            book = metadata_entry['book']\n",
        "            verse = metadata_entry.get('verse', '')\n",
        "            if verse and verse_counts[book] < 3:\n",
        "                if verse not in grouped_context_verses[book]:\n",
        "                    grouped_context_verses[book].append(verse)\n",
        "                    verse_counts[book] += 1\n",
        "    logger.info(f\"Grouped context verses for selected books: {grouped_context_verses}\")\n",
        "    handler.flush()\n",
        "    return grouped_context_verses\n",
        "\n",
        "def _parse_llm_response_and_print(answer):\n",
        "    \"\"\"Parses the LLM response and prints it to the console.\"\"\"\n",
        "    if answer.strip().startswith(\"This question is out of scope\"):\n",
        "        print(answer)\n",
        "        logger.info(f\"LLM response: {answer.strip()}\")\n",
        "        handler.flush()\n",
        "        return\n",
        "\n",
        "    pattern = r\"According to (.*?):\\nVerse: (.*?)\\nMeaning: (.*?)(?=(?:\\nAccording to | *$))\"\n",
        "    matches = re.findall(pattern, answer, re.DOTALL)\n",
        "\n",
        "    if matches:\n",
        "        for match in matches:\n",
        "            book_name = match[0].strip()\n",
        "            verse = match[1].strip()\n",
        "            meaning = match[2].strip()\n",
        "\n",
        "            print(f\"According to {book_name}:\")\n",
        "            print(f\"Verse: {verse}\")\n",
        "            print(f\"Meaning: {meaning}\")\n",
        "            logger.info(f\"Printed answer for {book_name}\")\n",
        "            handler.flush()\n",
        "    else:\n",
        "        print(answer)\n",
        "        logger.warning(f\"LLM response did not match expected pattern: {answer[:100]}...\")\n",
        "        handler.flush()\n",
        "\n",
        "def process_and_answer(question, selected_book_names, embeddings, metadata_dict, index, model_encoder, cache):\n",
        "    \"\"\"Process user input, perform search, and generate answer for all selected books.\"\"\"\n",
        "    cache_key = (question, tuple(sorted(selected_book_names)))\n",
        "    if cache_key in cache:\n",
        "        logger.info(f\"Serving answer from cache for question: {question[:50]}...\")\n",
        "        handler.flush()\n",
        "        answer = cache[cache_key]\n",
        "    else:\n",
        "        # Removed: OUT_OF_SCOPE_KEYWORDS check from here as it's now handled in ask_question\n",
        "        D, I = _perform_semantic_search(question, model_encoder, index)\n",
        "        grouped_context_verses = _group_context_verses(I[0], selected_book_names, metadata_dict)\n",
        "        answer = get_llm_response(question, grouped_context_verses)\n",
        "        cache[cache_key] = answer\n",
        "        save_cache(cache)\n",
        "\n",
        "    _parse_llm_response_and_print(answer)\n",
        "\n",
        "def ask_question(): # No parameters needed, relies on globals\n",
        "    \"\"\"Ask questions, relying on globally loaded data and model.\"\"\"\n",
        "    print(\"I am WiseBot, your philosophical companion.\")\n",
        "    print(\"Ask me any question and gain Wisdom!\")\n",
        "\n",
        "    while True:\n",
        "        question = _get_user_question_and_handle_exit() # Get question first\n",
        "        if question is None: # User exited from question prompt\n",
        "            logger.info(\"User quit the application.\")\n",
        "            handler.flush()\n",
        "            break\n",
        "\n",
        "        # NEW: Perform out-of-scope check immediately after getting the question\n",
        "        is_out_of_scope_by_keywords = False\n",
        "        for keyword in OUT_OF_SCOPE_KEYWORDS:\n",
        "            if keyword in question.lower():\n",
        "                is_out_of_scope_by_keywords = True\n",
        "                break\n",
        "\n",
        "        if is_out_of_scope_by_keywords:\n",
        "            print(\"This question is out of scope for my purpose.\")\n",
        "            logger.info(f\"Question identified as out of scope by keywords: {question[:50]}...\")\n",
        "            handler.flush()\n",
        "            continue # Loop back to ask a new question, bypassing book choice\n",
        "\n",
        "        # Print book selection menu once a valid question is entered\n",
        "        print(\"Choose books by entering numbers separated by commas (e.g., 1,2 for Gita and Quran):\")\n",
        "        print(\"1. Gita\")\n",
        "        print(\"2. Quran\")\n",
        "        print(\"3. Bible\")\n",
        "\n",
        "        # If question is in scope, proceed to get book choices\n",
        "        selected_book_names = _get_book_choices_and_handle_exit()\n",
        "        if selected_book_names is None: # User exited from book choice prompt\n",
        "            logger.info(\"User quit the application during book choice.\")\n",
        "            handler.flush()\n",
        "            break\n",
        "        # If selected_book_names is an empty list, it means invalid input, the loop in _get_book_choices_and_handle_exit\n",
        "        # would re-prompt. So, if we get here with an empty list, it implies no books were validly selected after retries,\n",
        "        # which means _get_book_choices_and_handle_exit would just keep prompting.\n",
        "\n",
        "        process_and_answer(question, selected_book_names, embeddings, metadata_dict, index, model_encoder, cache)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Application started.\")\n",
        "    handler.flush()\n",
        "    ask_question()\n",
        "    logger.info(\"Application ended.\")\n",
        "    handler.flush()\n",
        "    handler.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am WiseBot, your philosophical companion.\n",
            "Ask me any question and gain Wisdom!\n",
            "You: what is pain\n",
            "Choose books by entering numbers separated by commas (e.g., 1,2 for Gita and Quran):\n",
            "1. Gita\n",
            "2. Quran\n",
            "3. Bible\n",
            "Enter your choices (e.g., 1,2 for Gita and Quran): 1\n",
            "According to Gita:\n",
            "Verse: It is said the fruit of actions performed in the mode of passion result in pain, while those performed in the mode of ignorance result in darkness.\n",
            "Meaning: Pain arises not just from external circumstances, but also from our own actions, particularly those driven by passion (intense desire and attachment) or ignorance (lack of wisdom and understanding). These actions lead to suffering.\n",
            "Example: A student who cheats on an exam out of passion for a good grade, or out of ignorance about the value of learning, might face the pain of guilt, potential expulsion, or a lack of genuine knowledge if caught.\n",
            "You: what is the meaning of life\n",
            "Choose books by entering numbers separated by commas (e.g., 1,2 for Gita and Quran):\n",
            "1. Gita\n",
            "2. Quran\n",
            "3. Bible\n",
            "Enter your choices (e.g., 1,2 for Gita and Quran): sdc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Invalid book choice input detected: sdc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid input format.\n",
            "Enter your choices (e.g., 1,2 for Gita and Quran): 1,2,3\n",
            "According to Gita:\n",
            "Verse: I am the Supreme Goal of all living beings, and I am also their Sustainer, Master, Witness, Abode, Shelter, and Friend. I am the Origin, End, and Resting Place of creation; I am the Repository and Eternal Seed.\n",
            "Meaning: Life's meaning is found in recognizing and connecting with the divine, which is the source and sustainer of all existence. The ultimate purpose is to realize this divine connection.\n",
            "Example: A person dedicates their life to selfless service and spiritual practice, seeking to understand and align themselves with a higher purpose beyond their individual desires.\n",
            "According to Quran:\n",
            "Verse: And nothing is the life of this world but a play and a passing delight; and the life in the hereafter is by far the better for all who are conscious of God.\n",
            "Meaning: The true meaning and purpose of life lies not in the temporary pleasures of this world, but in preparing for the eternal life in the hereafter through consciousness of God and righteous living.\n",
            "Example: Someone who chooses to live a modest life, focusing on prayer and good deeds, rather than accumulating wealth and seeking fleeting worldly joys, because they believe in a greater, lasting reality.\n",
            "According to Bible:\n",
            "Verse: It is the spirit who gives life. The flesh profits nothing.\n",
            "Meaning: The true essence of life and its ultimate meaning are spiritual, not material. Focusing on spiritual growth and connection with the divine is what truly gives life purpose and substance.\n",
            "Example: A person facing hardship chooses to find strength and purpose in their faith and inner spiritual resolve, rather than solely in their physical comfort or material possessions.\n",
            "You: can you tell me a joke\n",
            "This question is out of scope for my purpose.\n",
            "You: exit\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}